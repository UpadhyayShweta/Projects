# -*- coding: utf-8 -*-
"""Search_Engine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12pIv2Z4VejmP7qtPrwn3yueUqNGXOoyc

# Search Engine for Clinical Trial
"""

import warnings
warnings.filterwarnings('ignore')

import re
import string
import pandas as pd
import numpy as np
from numpy import dot
from numpy.linalg import norm
from matplotlib import pyplot
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from pathlib import Path
import streamlit as st

import gensim
from gensim.models import Word2Vec
from gensim.models import FastText

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords # used for preprocessing
from nltk.stem import WordNetLemmatizer # used for preprocessing
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')


#load the dataset
#filepath = '/Data/Covid19_clinical_trials.csv'
trial_data=pd.read_csv('/Data/Covid19_clinical_trials.csv')
trial_data_copy = trial_data.copy()

### Data Preprocessing

# Function to lemmatize Words
lemmatizer = WordNetLemmatizer()
def lemmatize(text):
    text = [lemmatizer.lemmatize(token) for token in text]
    return text

#function for data preprocessing of texts
def pre_processing(text):

  text = text.lower()
  text = ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",str(text)).split()) #remove urls
  text = re.sub(r'\d+', '', str(text)) #emove no.s
  text=text.replace('\n',' ')
  text = word_tokenize(text)
  text = [char for char in text if char not in string.punctuation]
  text = [word for word in text if word not in stopwords.words('english')]
  text = lemmatize(text)
  text = ' '.join(text)
  return text

skipgram = Word2Vec.load('/Data/Output/skipgram.bin')
fastText = Word2Vec.load('/Data/Output/FastText.bin')

#Function to take average of all vectors of each word in abstract, so that each abstract can represent (1,100) dimensional vector
def get_mean_vector(word2vec_model, words):
    # remove oov words
    words = [word for word in word_tokenize(words) if word in list(word2vec_model.wv.index_to_key)] #if word is in vocab
    if len(words) >= 1:
        return np.mean(word2vec_model.wv[words], axis=0)
    else:
        return np.array([0]*100)



#defining function to define cosine similarity
def cos_sim(a,b):

    return dot(a, b)/(norm(a)*norm(b))

#Preprocessing input, because input should be in same form as training data set
def preprocessing_input(query,word2vec_model):
    query=pre_processing(query)
    wv=get_mean_vector(word2vec_model,query)


    return wv

#pd.set_option("display.max_colwidth")       #this function will display full text from each column

#function to return top n similar results
def top_n(query,p,df,model):
    print(query)

    query=preprocessing_input(query,model)    #preprocessing input to list of vectors

    x=[]
    #Converting cosine similarities of overall data set with input queries into LIST
    for i in range(len(p)):
        x.append(cos_sim(query,p[i]))


 #store list in tmp to retrieve index
    tmp=list(x)

 #sort list so that largest elements are on the far right

    res = sorted(range(len(x)), key = lambda sub: x[sub])[-10:]
    sim=[tmp[i] for i in reversed(res)]
    print(sim)
 #get index of the 10 or n largest element
    L=[]
    for i in reversed(res):

        L.append(i)
    return df.iloc[L, [1,2,5,6]],sim  #only returning dataframe with id,title,abstract ,publication date

### Search Results

#Loading our pretrained vectors of each abstract in skipgram model
sg_vec = pd.read_csv('Data/Output/skipgram_abstract_vec.csv')
skipgram_vectors=[]
for i in range(trial_data_copy.shape[0]):
    skipgram_vectors.append(sg_vec[str(i)].values)


#Loading our pretrained vectors of each abstract in FastText model
ft_vec = pd.read_csv('/Data/Output/fastText_abstract_vec.csv')
FastText_vectors=[]
for i in range(trial_data_copy.shape[0]):
    FastText_vectors.append(ft_vec[str(i)].values)


#streamlit function
def main():
    # Load data and models
    data = trial_data_copy     #our data which we have to display

    st.title("Search engine")      #title of our app
    #st.write('Select Model')


    Vectors = st.selectbox('Select Model',options=['Skipgram' , 'Fasttext'])
    if Vectors=='Skipgram':
        wv=skipgram_vectors
        word2vec_model=skipgram
    elif Vectors=='Fasttext':
        wv=FastText_vectors
        word2vec_model=fastText
    # User search
    #st.write('Type your query here') 
    user_input = st.text_input('Type your query here')   #getting input from user


    # Fetch results
    if user_input:

        P,sim =top_n(str(user_input),wv,data,word2vec_model)     #storing our output dataframe in P
        #Plotly function to display our dataframe in form of plotly table
        fig = go.Figure(data=[go.Table(header=dict(values=['ID', 'Title','Abstract','Publication Date','Similarity']),cells=dict(values=[list(P['Trial ID'].values),list(P['Title'].values), list(P['Abstract'].values),list(P['Publication date'].values),list(np.around(sim,4))],align='center'))])
        #displying our plotly table
        fig.update_layout(height=1700)
        st.plotly_chart(fig)
        

if __name__ == "__main__":
    main()

